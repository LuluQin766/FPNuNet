# image_mask_dataset_h5.py
import os
import h5py
import torch
from torch.utils.data import Dataset
import numpy as np
from pytorch_lightning import LightningDataModule
from torch.utils.data import Dataset, DataLoader
from typing import Dict, List, Tuple, Optional, Union
import albumentations
import cv2
from scipy import ndimage
from skimage import morphology as morph

DEBUG = False
# DEBUG = True    # True for printing debug statements

def get_bounding_box(img):
    """Get bounding box coordinates of a binary image."""
    rows = np.any(img, axis=1)
    cols = np.any(img, axis=0)
    rmin, rmax = np.where(rows)[0][[0, -1]]
    cmin, cmax = np.where(cols)[0][[0, -1]]
    return rmin, rmax, cmin, cmax

def fix_mirror_padding(ann):
    """Fix mirror padding in annotation."""
    return ann

def cropping_center(img, crop_shape):
    """Crop center of image to specified shape."""
    h, w = img.shape[:2]
    crop_h, crop_w = crop_shape[:2]
    start_h = (h - crop_h) // 2
    start_w = (w - crop_w) // 2
    return img[start_h:start_h+crop_h, start_w:start_w+crop_w]

def gen_instance_hv_map(ann: np.ndarray, crop_shape: tuple = None) -> np.ndarray:
    """
    Generate horizontal and vertical (HoVer) maps from instance segmentation.

    Args:
        ann (ndarray): Instance annotation map (H, W). 
                        Each pixel value represents the instance ID. the background is 0. 
                        the shape is the same as the input image.
        crop_shape (tuple): Shape (h, w) for cropping center.

    Returns:
        ndarray: HoVer map with 2 channels [H-channel, V-channel], shape (H, W, 2).
    """
    orig_ann = ann.copy()   # instance ID map
    fixed_ann = fix_mirror_padding(orig_ann)
    # re-cropping with fixed instance id map
    if crop_shape is not None:
        crop_ann = cropping_center(fixed_ann, crop_shape)
    else:
        crop_ann = fixed_ann
    # remove small objects
    crop_ann = morph.remove_small_objects(crop_ann, min_size=30)

    x_map = np.zeros_like(orig_ann, dtype=np.float32)
    y_map = np.zeros_like(orig_ann, dtype=np.float32)

    inst_ids = np.unique(crop_ann)
    inst_ids = inst_ids[inst_ids != 0]  # skip background

    for inst_id in inst_ids:
        inst_mask = (fixed_ann == inst_id).astype(np.uint8)
        bbox = get_bounding_box(inst_mask)

        # Expand box with clipping
        # expand the box by 2px
        # Because we first pad the ann at line 207, the bboxes
        # will remain valid after expansion
        y1 = max(bbox[0] - 2, 0)
        y2 = min(bbox[1] + 2, inst_mask.shape[0])
        x1 = max(bbox[2] - 2, 0)
        x2 = min(bbox[3] + 2, inst_mask.shape[1])
        inst_crop = inst_mask[y1:y2, x1:x2]

        if inst_crop.shape[0] < 2 or inst_crop.shape[1] < 2:
            continue

        com = ndimage.center_of_mass(inst_crop)
        if np.any(np.isnan(com)):
            continue
        cy, cx = int(round(com[0])), int(round(com[1]))

        yy, xx = np.meshgrid(np.arange(inst_crop.shape[1]), np.arange(inst_crop.shape[0]))
        xx = (xx - cx).astype(np.float32)
        yy = (yy - cy).astype(np.float32)

        # normalize to [-1, 1]
        if np.any(xx > 0): xx[xx > 0] /= xx[xx > 0].max()
        if np.any(xx < 0): xx[xx < 0] /= -xx[xx < 0].min()
        if np.any(yy > 0): yy[yy > 0] /= yy[yy > 0].max()
        if np.any(yy < 0): yy[yy < 0] /= -yy[yy < 0].min()

        xx[inst_crop == 0] = 0
        yy[inst_crop == 0] = 0

        x_map[y1:y2, x1:x2][inst_crop > 0] = xx[inst_crop > 0]
        y_map[y1:y2, x1:x2][inst_crop > 0] = yy[inst_crop > 0]

    return np.stack([x_map, y_map], axis=-1)

class H5ImageMaskHVDataset(Dataset):
    def __init__(self, 
                 h5_file_data: h5py.File, # Path to the HDF5 file
                 img_list: list, # list of image ids
                 augmentation = None, # albumentations.Compose
                 dataset_mean=(0.485, 0.456, 0.406),
                 dataset_std=(0.229, 0.224, 0.225),
                 ignored_classes=None, # only supports None, 0 or [0, ...]
                 return_img_id=True, # Whether to return the patch id along with the sample
                 output_aux_tokens=False, # Whether to return auxiliary tokens for each sample
    ):
        """
        Args:
            h5_file_path (str): Path to the HDF5 file generated by produce_MoNuSAC_instance.py.
            data_type (str): Type of data to load. Can be "train", "val", or "test". 
                "train" includes validation data, 80% for training and 20% for validation
            csv_file_path (str, optional): Path to CSV file with patch metadata.
                Expected columns include at least 'patch_id'.
            transform (callable, optional): Transformation to apply on the sample.
            return_img_id (bool): Whether to return the patch id along with the sample.
        """
        self.h5_file_data = h5_file_data
        self.img_list = img_list
        self.augmentation = augmentation

        if dataset_mean is not None:
            self.dataset_mean = np.array(dataset_mean, dtype=np.float32)
            self.dataset_std = np.array(dataset_std, dtype=np.float32)
        else:
            self.dataset_mean = None
            self.dataset_std = None

        self.ignored_classes = ignored_classes
        self.return_img_id = return_img_id
        self.output_aux_tokens = output_aux_tokens

    def __len__(self):
        return len(self.img_list)

    def __getitem__(self, idx):
        patch_id = self.img_list[idx]
        if DEBUG:
            print(f"\n\n ======== idx: {idx} ======== patch_id: {patch_id}")

        grp = self.h5_file_data[patch_id]
        image = np.array(grp['image'])                  # Expected shape: (H, W, C)
        inst_map = np.array(grp['inst_map'])            # Expected shape: (H, W)
        boundary_map = np.array(grp['boundary_map'])    # Expected shape: (H, W)
        type_map = np.array(grp['type_map'])            # Expected shape: (H, W)

        # Normalize image.
        image = image.astype(np.float32)
        if self.dataset_mean is not None:
            image = (image - self.dataset_mean) / self.dataset_std
        else:
            image = image / 255.0

        # Create combined mask for transformation (same as cd47nusc.py)
        mask = np.stack([inst_map, type_map], axis=-1)
        
        # Apply transforms if provided
        if self.augmentation is not None:
            transformed = self.augmentation(image=image, mask=mask)
            image = transformed["image"]
            mask = transformed["mask"]
        
        # Extract transformed maps (after transforms, same as cd47nusc.py)
        inst_map = mask[:, :, 0].copy()
        type_map = mask[:, :, 1].copy()
        
        # Generate binary map from instance map
        bin_map = np.zeros_like(inst_map, dtype=np.uint8)
        bin_map[inst_map > 0] = 1
        
        # Generate hv_map from inst_map after transforms (same as cd47nusc.py)
        hv_map = gen_instance_hv_map(inst_map)
            
        # Convert to torch tensors
        image = torch.from_numpy(image).permute(2, 0, 1).float()  # (C, H, W)
        bin_map = torch.from_numpy(np.expand_dims(bin_map, axis=0)).long()  # (1, H, W)
        boundary_map = torch.from_numpy(np.expand_dims(boundary_map, axis=0)).long()  # (H, W)
        inst_map = torch.from_numpy(inst_map).long()  # (H, W)
        type_map = torch.from_numpy(type_map).long()  # (H, W)
        hv_map = torch.from_numpy(hv_map).permute(2, 0, 1).float()  # (2, H, W)

        return_dict = {
            'image': image,
            'bin': bin_map,
            'boundary': boundary_map,
            'inst': inst_map,
            'hv': hv_map,
            'tp': type_map
        }

        if self.output_aux_tokens:
            inst_map_down2 = np.array(grp['inst_map_down2'])  # Expected shape: (H/2, W/2)
            inst_map_down4 = np.array(grp['inst_map_down4'])  # Expected shape: (H/4, W/4)

            # get binary auxiliary outputs from inst_map_down2 and inst_map_down4
            bin_aux_outs = [
                np.zeros_like(inst_map_down4, dtype=np.uint8),
                np.zeros_like(inst_map_down2, dtype=np.uint8)
            ]
            bin_aux_outs[0][inst_map_down4 > 0] = 1
            bin_aux_outs[1][inst_map_down2 > 0] = 1

            type_map_down2 = np.array(grp['type_map_down2'])  # Expected shape: (H/2, W/2)
            type_map_down4 = np.array(grp['type_map_down4'])  # Expected shape: (H/4, W/4)
            type_aux_outs = [type_map_down4, type_map_down2]

            hv_map_down2 = np.array(grp['hv_map_down2'])  # Expected shape: (H/2, W/2)
            hv_map_down4 = np.array(grp['hv_map_down4'])  # Expected shape: (H/4, W/4)
            hv_aux_outs = [hv_map_down4, hv_map_down2]

            for i in range(len(bin_aux_outs)):
                bin_aux_outs[i] = torch.from_numpy(bin_aux_outs[i]).long()
            for i in range(len(hv_aux_outs)):
                hv_aux_outs[i] = torch.from_numpy(hv_aux_outs[i]).permute(2, 0, 1).float()
            for i in range(len(type_aux_outs)):
                type_aux_outs[i] = torch.from_numpy(type_aux_outs[i]).long()

            return_dict.update({
                'bin_aux_outs': bin_aux_outs,
                'hv_aux_outs': hv_aux_outs,
                'type_aux_outs': type_aux_outs,
            })
        
        if self.return_img_id:
            return return_dict, patch_id
        else:
            return return_dict


class H5GeneralDataModule(LightningDataModule):
    """
    A PyTorch Lightning DataModule for HDF5-based image-mask datasets 
    with optional augmentations and HV map targets.
    """
    def __init__(self,
                 data_file_dict: Dict[str, str],
                 common_cfg_dict: Dict,
                 dataset_classs,  # Currently unused
                 augs_augmentation: Optional[Union[Tuple, List, object]],
                 batch_size: int,
                 num_workers: int,
                 output_aux_tokens: bool = False,  # Whether to return auxiliary tokens for each sample
                 debug_mode: bool = False,  # Debug mode: limit samples to 50
        ):
        super().__init__()
        self.data_file_dict = data_file_dict
        self.common_cfg_dict = common_cfg_dict
        self.dataset_class = H5ImageMaskHVDataset  # hardcoded here; could be replaced by dataset_classs
        self.augs_augmentation = augs_augmentation
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.output_aux_tokens = output_aux_tokens
        self.debug_mode = debug_mode

        # Internal holders
        self.train_dataset = None
        self.val_dataset = None
        self.test_dataset = None
        self.train_h5_data = None
        self.test_h5_data = None

    def setup(self, stage: Optional[str] = None):
        # Load HDF5 files
        self.train_h5_data = h5py.File(self.data_file_dict["train"], 'r')
        train_img_list = sorted(self.train_h5_data.keys())
        # print(f" --- Total number of training images: {len(train_img_list)}")

        # Debug mode: limit samples to 50
        if self.debug_mode:
            print(f"ğŸ› Debug mode enabled: limiting samples to 50")
            train_img_list = train_img_list[:50]
            print(f" --- Debug: Limited training images to {len(train_img_list)}")

        # Split training and validation sets (80% / 20%)
        num_train = int(len(train_img_list) * 0.8)
        train_list = train_img_list[:num_train]
        val_list = train_img_list[num_train:]
        
        if self.debug_mode:
            print(f" --- Debug: Training images: {len(train_list)}")
            print(f" --- Debug: Validation images: {len(val_list)}")
        else:
            # print(f" --- Number of training images: {len(train_list)}")
            # print(f" --- Number of validation images: {len(val_list)}")
            pass

        self.test_h5_data = h5py.File(self.data_file_dict["test"], 'r')
        test_list = sorted(self.test_h5_data.keys())
        
        if self.debug_mode:
            test_list = test_list[:50]  # Also limit test samples in debug mode
            print(f" --- Debug: Limited test images to {len(test_list)}")
        else:
            # print(f" --- Total number of testing images: {len(test_list)}")
            pass

        # Parse augmentation
        train_aug, val_aug = self._parse_augmentation(self.augs_augmentation)

        # Build datasets
        self.train_dataset = self.dataset_class(
            **self.common_cfg_dict,
            h5_file_data=self.train_h5_data,
            img_list=train_list,
            augmentation=train_aug,
            output_aux_tokens=self.output_aux_tokens)

        self.val_dataset = self.dataset_class(
            **self.common_cfg_dict,
            h5_file_data=self.train_h5_data,
            img_list=val_list,
            augmentation=val_aug,
            output_aux_tokens=self.output_aux_tokens)

        self.test_dataset = self.dataset_class(
            **self.common_cfg_dict,
            h5_file_data=self.test_h5_data,
            img_list=test_list,
            augmentation=val_aug,
            output_aux_tokens=self.output_aux_tokens)

    def train_dataloader(self):
        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)

    def val_dataloader(self):
        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)

    def test_dataloader(self):
        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)

    @staticmethod
    def _parse_augmentation(augs) -> Tuple[Optional[object], Optional[object]]:
        if augs is None:
            return None, None
        if isinstance(augs, (tuple, list)) and len(augs) == 2:
            return augs[0], augs[1]
        return augs, augs

def get_augmentation():
    W, H = (512, 512)
    transform_train_fn = albumentations.Compose([
        albumentations.RandomResizedCrop(H, W, scale=(0.08, 1.0), p=1.0, 
                                         interpolation=cv2.INTER_NEAREST),
        albumentations.Flip(p=0.75),
        albumentations.RandomRotate90(),
        albumentations.ColorJitter(0.1, 0.1, 0.1, 0.1),
    ])
    transform_test_fn = albumentations.Compose([
        albumentations.Resize(H, W, interpolation=cv2.INTER_NEAREST),
    ])
    return transform_train_fn, transform_test_fn

def get_data_module(cfg):
    data_file_dict = {
        "train": cfg.dataset.train_h5_file_path,
        "test": cfg.dataset.test_h5_file_path,
    }

    common_cfg_dict = {
        "dataset_mean": cfg.dataset.dataset_mean,
        "dataset_std": cfg.dataset.dataset_std,
        "ignored_classes": cfg.dataset.ignored_classes,  # only supports None, 0 or [0, ...]
    }

    print(" --- Using image input with transforms support")
    # === åˆå§‹åŒ– DataModule ===
    data_module = H5GeneralDataModule(
        data_file_dict=data_file_dict,
        common_cfg_dict=common_cfg_dict,
        dataset_classs = cfg.dataset.num_classes, 
        augs_augmentation=get_augmentation(),  # å¯ç”¨æ•°æ®å¢å¼º
        batch_size=cfg.batch_size, 
        num_workers=cfg.num_workers,
        output_aux_tokens=False,
    )
    
    return data_module

# Example usage:
if __name__ == "__main__":
    # Example file paths; adjust these to your actual paths.
    root_dir = "/root/public_data/a_SAMNuSeg_train_dataset/CD47_nuclei_patches_v4"

    # === æ¨¡æ‹Ÿé…ç½® ===
    data_file_dict = {
        "train": f"{root_dir}/DEBUG_gt_CD47_nuclei_train_256x256_128x128_img_inst_type_hv.h5",
        "test": f"{root_dir}/DEBUG_gt_CD47_nuclei_valid_256x256_128x128_img_inst_type_hv.h5"
    }

    common_cfg_dict = {
        "dataset_mean": None,
        "dataset_std": None,
        "ignored_classes": 0,  # only supports None, 0 or [0, ...]
    }

    # === åˆå§‹åŒ– DataModule ===
    datamodule = H5GeneralDataModule(
        data_file_dict=data_file_dict,
        common_cfg_dict=common_cfg_dict,
        dataset_classs=4,  # å½“å‰ä»£ç æœªä½¿ç”¨
        augs_augmentation=get_augmentation(),
        batch_size=2,
        num_workers=0  # æµ‹è¯•æ—¶ç”¨0é¿å…å¤šè¿›ç¨‹é”™è¯¯
    )

    # === è®¾ç½®é˜¶æ®µ ===
    datamodule.setup()

    # === è·å– DataLoaders ===
    train_loader = datamodule.train_dataloader()
    val_loader = datamodule.val_dataloader()
    test_loader = datamodule.test_dataloader()

    # === æµ‹è¯•è¾“å‡º ===
    print("\nğŸš€ Test: iterate over one batch from train/val/test loaders")
    for name, loader in zip(['Train', 'Val', 'Test'], [train_loader, val_loader, test_loader]):
        print(f"\n==> {name} batch:")
        for (input_dict, batch_id) in loader:
            for k, v in input_dict.items():
                if isinstance(v, (list, tuple)):
                    print(f" ---- {k}: len={len(v)}")
                    for i, vi in enumerate(v):
                        print(f"  {k}[{i}]: shape={vi.shape}, dtype={vi.dtype}")
                else:
                    print(f" ---- {k}: shape={v.shape}, dtype={v.dtype}")
            break  # åªå–1ä¸ªbatchæµ‹è¯•

    # # === å¯é€‰ï¼šå¯è§†åŒ–ç¬¬ä¸€ä¸ªæ ·æœ¬ ===
    # for batch in train_loader:
    #     image = batch['image'][0].numpy().transpose(1, 2, 0)
    #     mask = batch['mask'][0].numpy().transpose(1, 2, 0) if batch['mask'].ndim == 4 else batch['mask'][0].numpy()
    #     plt.figure(figsize=(10, 4))
    #     plt.subplot(1, 2, 1)
    #     plt.title("Image")
    #     plt.imshow(image)
    #     plt.subplot(1, 2, 2)
    #     plt.title("HV Map")
    #     plt.imshow(mask[..., 0], cmap='jet')  # æ˜¾ç¤ºç¬¬ä¸€ä¸ªé€šé“
    #     plt.show()
    #     break

    print("\n --------- Done.\n\n")
