"""
è®­ç»ƒå‡½æ•°æ¨¡å—
åŒ…å«æ ¸å¿ƒçš„è®­ç»ƒé€»è¾‘ï¼Œå¯ä»¥è¢«å¤šä¸ªè¿è¡Œè„šæœ¬è°ƒç”¨
"""

import os
import time
import torch
from pytorch_lightning import Trainer
from pytorch_lightning.loggers import WandbLogger
from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint, EarlyStopping

from utils_run02 import get_data_module, get_pl_module
from get_model import get_model
from metrics_v21 import get_metrics


def save_model_config(cfg, run_name):
    """
    ä¿å­˜æ¨¡å‹é…ç½®ä¿¡æ¯åˆ°JSONæ–‡ä»¶
    
    Args:
        cfg: é…ç½®å¯¹è±¡
        run_name: è¿è¡Œåç§°
    """
    import json
    
    # åˆ›å»ºé…ç½®ä¿¡æ¯å­—å…¸
    config_info = {
        "run_name": run_name,
        "model": {
            "extra_type": cfg.model.extra_type,
            "extra_encoder": cfg.model.extra_encoder,
            "type": cfg.model.type,
            "checkpoint": cfg.model.checkpoint,
            "prompt_dim": cfg.model.prompt_dim,
            "freeze": {
                "image_encoder": cfg.model.freeze.image_encoder,
                "prompt_encoder": cfg.model.freeze.prompt_encoder,
                "mask_decoder": cfg.model.freeze.mask_decoder,
            }
        },
        "dataset": {
            "num_classes": cfg.dataset.num_classes,
            "image_hw": cfg.dataset.image_hw,
            "ignored_classes": cfg.dataset.ignored_classes,
        },
        "training": {
            "batch_size": cfg.batch_size,
            "num_epochs": cfg.opt.num_epochs,
            "learning_rate": cfg.opt.learning_rate,
            "weight_decay": cfg.opt.weight_decay,
            "precision": cfg.opt.precision,
            "unfreeze_epoch": cfg.opt.unfreeze_epoch if "unfreeze_epoch" in cfg.opt else None,
            "early_stopping_patience": cfg.opt.early_stopping_patience if "early_stopping_patience" in cfg.opt else None,
        },
        "devices": cfg.devices,
        "project": cfg.project,
        "seed": cfg.random_seed,
    }
    
    # ä¿å­˜åˆ°JSONæ–‡ä»¶
    config_file = os.path.join(cfg.log_dir, 'model_config.json')
    with open(config_file, 'w') as f:
        json.dump(config_info, f, indent=4)
    
    print(f" -------- Model config saved to: {config_file}")


def train_model(cfg, run_name=None):
    """
    è®­ç»ƒå•ä¸ªæ¨¡å‹çš„å‡½æ•°
    
    Args:
        cfg: é…ç½®å¯¹è±¡
        run_name: è¿è¡Œåç§°ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
        
    Returns:
        trainer: è®­ç»ƒå™¨å¯¹è±¡
        pl_module: PyTorch Lightningæ¨¡å—
    """
    print(f"\n\n ----------- Starting Training: {cfg.get('name', 'Unknown')} -----------")
    
    # è®¾ç½®è¿è¡Œåç§°å’Œæ—¥å¿—ç›®å½•
    if run_name is None:
        # ç›´æ¥ä½¿ç”¨cfg.nameï¼Œå®ƒå·²ç»åœ¨config_managerä¸­åŒ…å«äº†æ—¶é—´æˆ³
        run_name = cfg.name
    
    log_dir = os.path.join(cfg.out_dir, run_name)
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
    
    cfg.log_dir = log_dir
    cfg.log_train_images_path = os.path.join(log_dir, 'train_images')
    os.makedirs(cfg.log_train_images_path, exist_ok=True)
    cfg.log_val_images_path = os.path.join(log_dir, 'val_images')
    os.makedirs(cfg.log_val_images_path, exist_ok=True)
    cfg.log_test_images_path = os.path.join(log_dir, 'test_images')
    os.makedirs(cfg.log_test_images_path, exist_ok=True)
    
    print(f" -------- project_path: {cfg.out_dir}")
    print(f" -------- run_name: {run_name}")
    print(f" -------- log_dir: {log_dir}")
    print(f" -------- cfg.log_train_images_path: {cfg.log_train_images_path}")
    print(f" -------- cfg.log_val_images_path: {cfg.log_val_images_path}")
    print(f" -------- cfg.log_test_images_path: {cfg.log_test_images_path}")

    # å‡†å¤‡æ•°æ®æ¨¡å—
    data_module = get_data_module(cfg)

    # å‡†å¤‡æ¨¡å‹
    sam_model = get_model(cfg)

    print(f" âœ… [MEMORY] allocated: {torch.cuda.memory_allocated()/1e9:.2f} GB, \n"
          f" âœ…  [MEMORY] reserved: {torch.cuda.memory_reserved()/1e9:.2f} GB")

    # å‡†å¤‡æŒ‡æ ‡
    metrics = get_metrics(cfg.dataset.num_classes, ignore_index=cfg.dataset.ignored_classes)

    # è®¾ç½®è®­ç»ƒå’ŒéªŒè¯è¿‡ç¨‹
    # ä»é…ç½®ä¸­è·å–æ¨¡å—ç‰ˆæœ¬ï¼Œé»˜è®¤ä¸ºv21
    pl_module_version = getattr(cfg, 'pl_module_version', 'v21')
    
    print(f"\nğŸš€ å¼€å§‹åˆå§‹åŒ–è®­ç»ƒæ¨¡å—...")
    print(f"ğŸ“‹ é…ç½®ä¿¡æ¯:")
    print(f"   - å®éªŒåç§°: {cfg.get('name', 'Unknown')}")
    print(f"   - æ¨¡å—ç‰ˆæœ¬: {pl_module_version}")
    print(f"   - æ‰¹æ¬¡å¤§å°: {cfg.batch_size}")
    print(f"   - å­¦ä¹ ç‡: {cfg.opt.learning_rate}")
    print(f"   - æƒé‡è¡°å‡: {cfg.opt.weight_decay}")
    
    pl_module = get_pl_module(cfg, model=sam_model, metrics=metrics, version=pl_module_version)
    
    # æ³¨æ„ï¼šTensorBoard è®°å½•å·²åœ¨ pl_module_multiHead_v21.py ä¸­è‡ªå®šä¹‰å®ç°
    # è¿™é‡Œåªè®¾ç½® WandB loggerï¼Œé¿å…é‡å¤è®°å½•
    
    # è®¾ç½®WandB logger
    wandb_logger = WandbLogger(
        project=cfg.project, 
        name=run_name, 
        save_dir=cfg.log_dir, 
        log_model=True
    )
    wandb_logger.watch(sam_model, log='all', log_freq=10)
    print(f" -------- wandb_logger: {wandb_logger}")
    print(f" -------- wandb_logger.save_dir: {wandb_logger.save_dir}")
    print(f" -------- TensorBoard logs will be saved to: {cfg.log_dir}/tensorboard")

    # åªä½¿ç”¨ WandB logger
    loggers = [wandb_logger]
    
    # è®¾ç½®å­¦ä¹ ç‡ç›‘æ§
    lr_monitor = LearningRateMonitor(logging_interval='epoch')
    accumulate_grad_batches = cfg.accumulate_grad_batches if "accumulate_grad_batches" in cfg else 1

    # æ·»åŠ æ¨¡å‹æ£€æŸ¥ç‚¹å›è°ƒ
    ckpt_path = os.path.join(cfg.log_dir, 'model_checkpoints')
    if not os.path.exists(ckpt_path):
        os.makedirs(ckpt_path)
    model_checkpoint = ModelCheckpoint(
        dirpath=ckpt_path,  # ä¿å­˜æ¨¡å‹çš„ç›®å½•
        filename=run_name + '-{epoch}-{val_loss:.4f}',  # æ–‡ä»¶åæ ¼å¼
        monitor='val_loss',  # ç›‘æ§çš„æŒ‡æ ‡ 
        mode='min',  # ä¿å­˜æœ€ä½³æ¨¡å‹çš„æ¨¡å¼
        save_top_k=5,  # ä¿å­˜æœ€å¥½çš„5ä¸ªæ¨¡å‹
        save_last=True,  # æ€»æ˜¯ä¿å­˜æœ€åä¸€ä¸ªepochçš„æ¨¡å‹
        every_n_epochs=1  # æ¯ä¸ªepochéƒ½ä¿å­˜
    )
    
    print(f" -------- model_checkpoint.dirpath: {model_checkpoint.dirpath}")
    print(f" -------- cfg.devices: {cfg.devices}")

    # æ·»åŠ æ—©åœæœºåˆ¶å›è°ƒ
    early_stopping_patience = cfg.opt.early_stopping_patience if "early_stopping_patience" in cfg.opt else None
    callbacks = [lr_monitor, model_checkpoint]
    
    if early_stopping_patience is not None:
        early_stopping = EarlyStopping(
            monitor='val_loss',           # ç›‘æ§éªŒè¯æŸå¤±
            mode='min',                   # æŸå¤±è¶Šå°è¶Šå¥½
            patience=early_stopping_patience,  # è€å¿ƒå€¼
            min_delta=1e-4,              # æœ€å°æ”¹å–„é˜ˆå€¼
            verbose=True,                # æ‰“å°æ—©åœä¿¡æ¯
            strict=True,                 # ä¸¥æ ¼æ¨¡å¼
            check_finite=True,          # æ£€æŸ¥æœ‰é™å€¼
            stopping_threshold=None,    # åœæ­¢é˜ˆå€¼
            divergence_threshold=None,  # å‘æ•£é˜ˆå€¼
            check_on_train_epoch_end=False  # ä¸åœ¨è®­ç»ƒepochç»“æŸæ—¶æ£€æŸ¥
        )
        callbacks.append(early_stopping)
        print(f" -------- Early stopping enabled with patience: {early_stopping_patience}")
    else:
        print(f" -------- Early stopping disabled")

    # ä¿å­˜æ¨¡å‹é…ç½®ä¿¡æ¯
    save_model_config(cfg, run_name)

    # åˆ›å»ºè®­ç»ƒå™¨
    # æ³¨æ„ï¼šPyTorch Lightning é»˜è®¤ä¼šåœ¨å½“å‰ç›®å½•åˆ›å»º lightning_logs/ ç›®å½•
    # ä½†æˆ‘ä»¬çš„æ¨¡å‹æ£€æŸ¥ç‚¹éƒ½ä¿å­˜åœ¨ model_checkpoints/ ç›®å½•ä¸­
    trainer = Trainer(
        default_root_dir=cfg.log_dir, 
        logger=loggers,  # ä½¿ç”¨åˆå¹¶çš„loggers
        devices=cfg.devices,    # e.g., [2,3] or just 4
        max_epochs=cfg.opt.num_epochs,
        accelerator="gpu", 
        strategy="auto",
        log_every_n_steps=5,   # Log metrics every 5 global steps
        num_sanity_val_steps=0,
        precision=cfg.opt.precision,
        callbacks=callbacks,  # ä½¿ç”¨åŒ…å«æ—©åœæœºåˆ¶çš„å›è°ƒåˆ—è¡¨
        accumulate_grad_batches=accumulate_grad_batches,
        fast_dev_run=False
    )
    
    print(f" -------- trainer.default_root_dir: {trainer.default_root_dir}")

    # å¼€å§‹è®­ç»ƒ
    trainer.fit(pl_module, data_module)
    
    # ç¡®ä¿ TensorBoard writer è¢«æ­£ç¡®å…³é—­
    if hasattr(pl_module, 'writer') and pl_module.writer is not None:
        pl_module.writer.close()
        print(f" âœ… TensorBoard writer closed for {run_name}")
    
    print(f" âœ… Training completed for {run_name}")
    return trainer, pl_module


def setup_environment():
    """
    è®¾ç½®è®­ç»ƒç¯å¢ƒ
    """
    import os
    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
    # os.environ["CUDA_VISIBLE_DEVICES"] = "1"  # ç°åœ¨é€šè¿‡ --devices å‚æ•°æ§åˆ¶
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:64"
    os.environ["WANDB_MODE"]="offline"
    
    import torch
    torch.set_float32_matmul_precision('high')  # æˆ– 'medium'
    
    print("\n âœ… Environment setup completed")


def print_config_summary(cfg):
    """
    æ‰“å°é…ç½®æ‘˜è¦
    """
    print("\n -------- Configuration Summary:")
    for k, v in cfg.items():
        if isinstance(v, dict):
            print(f"  ---- {k}:")
            for kk, vv in v.items():
                print(f"   {kk}: {vv}")
        else:
            print(f"  ---- {k}: {v}")
    print("\n")
