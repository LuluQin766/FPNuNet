"""
å·¥å…·å‡½æ•°æ¨¡å—
ç»Ÿä¸€ç®¡ç†æ•°æ®æ¨¡å—å’ŒPyTorch Lightningæ¨¡å—çš„åˆ›å»ºå‡½æ•°
æ”¯æŒå¤šç‰ˆæœ¬æ¨¡å—é€‰æ‹©
"""

from h5dataloder_v21 import H5GeneralDataModule

# åŠ¨æ€å¯¼å…¥ PanNuke æ•°æ®åŠ è½½å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
try:
    from h5dataloder_pannuke import PanNukeMultiFoldDataModule, get_pannuke_fold_datamodules, get_augmentation as get_pannuke_augmentation
    PANNUKE_AVAILABLE = True
except ImportError:
    PANNUKE_AVAILABLE = False
    print("âš ï¸  PanNuke data loader not available")

# åŠ¨æ€å¯¼å…¥ä¸åŒç‰ˆæœ¬çš„æ¨¡å—
def _check_config_completeness(cfg, version):
    """
    æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦åŒ…å«ç‰ˆæœ¬æ‰€éœ€çš„å‚æ•°
    
    Args:
        cfg: é…ç½®å¯¹è±¡
        version (str): ç‰ˆæœ¬å·
        
    Returns:
        bool: é…ç½®æ˜¯å¦å®Œæ•´
    """
    print(f"\nğŸ” æ£€æŸ¥ {version} ç‰ˆæœ¬é…ç½®å®Œæ•´æ€§...")
    
    missing_params = []
    warnings = []
    
    if version == "v21":
        # v21ç‰ˆæœ¬éœ€è¦çš„å‚æ•°
        required_params = [ 
            ("opt.learning_rate", "å­¦ä¹ ç‡"),
            ("opt.weight_decay", "æƒé‡è¡°å‡"),
            ("opt.steps", "å­¦ä¹ ç‡è°ƒåº¦æ­¥æ•°"),
            ("opt.warmup_steps", "é¢„çƒ­æ­¥æ•°"),
        ]
        
        for param_path, param_name in required_params:
            if not _has_nested_attr(cfg, param_path):
                missing_params.append(f"   âŒ {param_name} ({param_path})")
            else:
                print(f"   âœ… {param_name}: {_get_nested_attr(cfg, param_path)}")
                
    elif version == "v22":
        # v22ç‰ˆæœ¬éœ€è¦çš„å‚æ•°
        required_params = [
            ("opt.learning_rate", "å­¦ä¹ ç‡"),
            ("opt.weight_decay", "æƒé‡è¡°å‡"),
            ("opt.warmup_steps", "é¢„çƒ­æ­¥æ•°"),
        ]
        
        optional_params = [
            ("opt.gradient_clip_val", "æ¢¯åº¦è£å‰ªå€¼"),
            ("opt.scheduler", "è°ƒåº¦å™¨ç±»å‹"),
        ]
        
        # æ£€æŸ¥å¿…éœ€å‚æ•°
        for param_path, param_name in required_params:
            if not _has_nested_attr(cfg, param_path):
                missing_params.append(f"   âŒ {param_name} ({param_path})")
            else:
                print(f"   âœ… {param_name}: {_get_nested_attr(cfg, param_path)}")
        
        # æ£€æŸ¥å¯é€‰å‚æ•°
        for param_path, param_name in optional_params:
            if not _has_nested_attr(cfg, param_path):
                warnings.append(f"   âš ï¸ {param_name} ({param_path}) - å°†ä½¿ç”¨é»˜è®¤å€¼")
            else:
                print(f"   âœ… {param_name}: {_get_nested_attr(cfg, param_path)}")
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«v21çš„å‚æ•°ï¼ˆå¯èƒ½ä¸éœ€è¦ï¼‰
        if _has_nested_attr(cfg, "opt.steps"):
            warnings.append(f"   âš ï¸ opt.steps - v22ç‰ˆæœ¬ä¸éœ€è¦æ­¤å‚æ•°ï¼Œå°†è¢«å¿½ç•¥")
    
    # è¾“å‡ºç»“æœ
    if missing_params:
        print(f"\nâŒ é…ç½®ä¸å®Œæ•´ï¼Œç¼ºå°‘ä»¥ä¸‹å¿…éœ€å‚æ•°:")
        for param in missing_params:
            print(param)
        print(f"\nğŸ’¡ è¯·æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦æ­£ç¡®è®¾ç½®äº† {version} ç‰ˆæœ¬æ‰€éœ€çš„å‚æ•°")
        return False
    
    if warnings:
        print(f"\nâš ï¸ é…ç½®è­¦å‘Š:")
        for warning in warnings:
            print(warning)
    
    print(f"âœ… {version} ç‰ˆæœ¬é…ç½®æ£€æŸ¥å®Œæˆ")
    return True


def _has_nested_attr(obj, attr_path):
    """æ£€æŸ¥åµŒå¥—å±æ€§æ˜¯å¦å­˜åœ¨"""
    try:
        attrs = attr_path.split('.')
        current = obj
        for attr in attrs:
            current = getattr(current, attr)
        return True
    except (AttributeError, KeyError):
        return False


def _get_nested_attr(obj, attr_path):
    """è·å–åµŒå¥—å±æ€§çš„å€¼"""
    try:
        attrs = attr_path.split('.')
        current = obj
        for attr in attrs:
            current = getattr(current, attr)
        return current
    except (AttributeError, KeyError):
        return None


def _import_pl_module(version="v21"):
    """
    æ ¹æ®ç‰ˆæœ¬å·åŠ¨æ€å¯¼å…¥å¯¹åº”çš„PyTorch Lightningæ¨¡å—
    
    Args:
        version (str): ç‰ˆæœ¬å·ï¼Œæ”¯æŒ "v21" æˆ– "v22"
        
    Returns:
        SamSegMultiHead: å¯¹åº”ç‰ˆæœ¬çš„æ¨¡å—ç±»
    """
    if version == "v21":
        from pl_module_multiHead_v21 import SamSegMultiHeadV6 as SamSegMultiHead
        print(f"\nğŸ“¦ ä½¿ç”¨ PyTorch Lightning æ¨¡å—ç‰ˆæœ¬: {version}")
        print(f"ğŸ¯ å­¦ä¹ ç‡è°ƒåº¦æ–¹æ³•: LambdaLR (å›ºå®šæ­¥æ•°è¡°å‡)")
        print(f"   - æ”¯æŒå‚æ•°: steps, warmup_steps")
        print(f"   - è°ƒåº¦æ–¹å¼: warmup â†’ 1.0 â†’ 0.1 â†’ 0.01\n")
        return SamSegMultiHead
    elif version == "v22":
        from pl_module_multiHead_v22 import SamSegMultiHeadV6 as SamSegMultiHead
        print(f"\nğŸ“¦ ä½¿ç”¨ PyTorch Lightning æ¨¡å—ç‰ˆæœ¬: {version}")
        print(f"ğŸ¯ å­¦ä¹ ç‡è°ƒåº¦æ–¹æ³•: CosineAnnealingLR + Warmup (ä½™å¼¦é€€ç«)")
        print(f"   - æ”¯æŒå‚æ•°: warmup_steps, gradient_clip_val, scheduler")
        print(f"   - è°ƒåº¦æ–¹å¼: warmup â†’ ä½™å¼¦é€€ç«è¡°å‡\n")
        return SamSegMultiHead
    else:
        raise ValueError(f"\nâŒ ä¸æ”¯æŒçš„ç‰ˆæœ¬: {version}ã€‚æ”¯æŒçš„ç‰ˆæœ¬: v21, v22\n")


def get_data_module(cfg):
    """
    åˆ›å»ºç»Ÿä¸€çš„æ•°æ®æ¨¡å—
    
    Args:
        cfg: é…ç½®å¯¹è±¡ï¼ŒåŒ…å«æ•°æ®é›†è·¯å¾„å’Œå‚æ•°
        
    Returns:
        DataModule: é…ç½®å¥½çš„æ•°æ®æ¨¡å—ï¼ˆH5GeneralDataModule æˆ– PanNukeDataModuleï¼‰
    """
    
    # æ£€æµ‹æ˜¯å¦ä½¿ç”¨ PanNuke æ•°æ®é›†
    use_pannuke = (hasattr(cfg.dataset, 'pannuke_dataloader') and 
                   cfg.dataset.pannuke_dataloader == "pannuke" and
                   PANNUKE_AVAILABLE)
    
    if use_pannuke:
        # ä½¿ç”¨ PanNuke æ•°æ®åŠ è½½å™¨
        print("\nğŸ“Š Using PanNuke data loader")
        
        # æ£€æµ‹debugæ¨¡å¼
        debug_mode = getattr(cfg, 'debug_mode', False)
        
        # ä½¿ç”¨é»˜è®¤çš„ fold3
        from h5dataloder_pannuke import PanNukeDataModule, get_augmentation
        data_module = PanNukeDataModule(
            data_root=cfg.dataset.pannuke_data_root,
            split_name='fold3',
            augmentation=get_augmentation(),
            batch_size=cfg.batch_size,
            num_workers=cfg.num_workers,
            dataset_mean=cfg.dataset.dataset_mean,
            dataset_std=cfg.dataset.dataset_std,
            output_aux_tokens=False,
            debug_mode=debug_mode,
        )
        return data_module
    
    # ä½¿ç”¨ä¼ ç»Ÿçš„ H5 æ•°æ®åŠ è½½å™¨
    print("\nğŸ“Š Using H5 data loader")
    data_file_dict = {
        "train": cfg.dataset.train_h5_file_path,
        "test": cfg.dataset.test_h5_file_path,
    }

    common_cfg_dict = {
        "dataset_mean": cfg.dataset.dataset_mean,
        "dataset_std": cfg.dataset.dataset_std,
        "ignored_classes": cfg.dataset.ignored_classes,  # only supports None, 0 or [0, ...]
    }

    # æ£€æµ‹debugæ¨¡å¼ï¼šå¦‚æœé…ç½®æ–‡ä»¶åä»¥"debug"ç»“å°¾ï¼Œåˆ™å¯ç”¨debugæ¨¡å¼
    debug_mode = False
    
    # æ–¹æ³•1: æ£€æŸ¥cfg.debug_modeé…ç½®
    if hasattr(cfg, 'debug_mode') and cfg.debug_mode:
        debug_mode = True
        print(f"ğŸ› Debug mode enabled from config.debug_mode")
    
    # æ–¹æ³•2: æ£€æŸ¥é…ç½®æ–‡ä»¶åï¼ˆé€šè¿‡æ£€æŸ¥æ•°æ®è·¯å¾„ä¸­çš„æ–‡ä»¶åï¼‰
    elif (hasattr(cfg.dataset, 'train_h5_file_path') and 
          cfg.dataset.train_h5_file_path is not None and 
          'debug' in cfg.dataset.train_h5_file_path.lower()):
        debug_mode = True
        print(f"ğŸ› Debug mode detected from dataset path containing 'debug'")
    
    # æ–¹æ³•3: æ£€æŸ¥å®éªŒåç§°æ˜¯å¦åŒ…å«debug
    elif hasattr(cfg, 'name') and cfg.name is not None and 'debug' in cfg.name.lower():
        debug_mode = True
        print(f"ğŸ› Debug mode detected from experiment name: {cfg.name}")
    
    if debug_mode:
        print(" --- Debug mode: limiting samples to 50 for faster testing")
    else:
        print(" --- Using image input")
    
    # === åˆå§‹åŒ– DataModule ===
    data_module = H5GeneralDataModule(
        data_file_dict=data_file_dict,
        common_cfg_dict=common_cfg_dict,
        dataset_classs=cfg.dataset.num_classes, 
        augs_augmentation=None,
        batch_size=cfg.batch_size, 
        num_workers=cfg.num_workers,
        output_aux_tokens=False,
        debug_mode=debug_mode,
    )
    
    return data_module


def get_pl_module(cfg, model, metrics, version="v21"):
    """
    åˆ›å»ºç»Ÿä¸€çš„PyTorch Lightningæ¨¡å—
    
    Args:
        cfg: é…ç½®å¯¹è±¡
        model: SAMæ¨¡å‹
        metrics: æŒ‡æ ‡é›†åˆ
        version (str): æ¨¡å—ç‰ˆæœ¬ï¼Œæ”¯æŒ "v21" æˆ– "v22"ï¼Œé»˜è®¤ä¸º "v21"
        
    Returns:
        SamSegMultiHead: é…ç½®å¥½çš„PyTorch Lightningæ¨¡å—
    """
    # æ£€æŸ¥é…ç½®ä¸­æ˜¯å¦æŒ‡å®šäº†ç‰ˆæœ¬
    config_version = getattr(cfg, 'pl_module_version', None)
    if config_version and config_version != version:
        print(f"âš ï¸ é…ç½®æ–‡ä»¶ä¸­æŒ‡å®šçš„ç‰ˆæœ¬ ({config_version}) ä¸å‚æ•°ç‰ˆæœ¬ ({version}) ä¸ä¸€è‡´")
        print(f"   ä½¿ç”¨å‚æ•°ç‰ˆæœ¬: {version}")
    
    # æ£€æŸ¥é…ç½®å®Œæ•´æ€§
    if not _check_config_completeness(cfg, version):
        raise ValueError(f"é…ç½®æ–‡ä»¶ä¸å®Œæ•´ï¼Œæ— æ³•åˆ›å»º {version} ç‰ˆæœ¬çš„æ¨¡å—")
    
    # åŠ¨æ€å¯¼å…¥å¯¹åº”ç‰ˆæœ¬çš„æ¨¡å—
    SamSegMultiHead = _import_pl_module(version)
    
    # åˆ›å»ºæ¨¡å—å®ä¾‹
    pl_module = SamSegMultiHead(
        cfg=cfg,
        sam_model=model,
        metrics=metrics,
        num_classes=cfg.dataset.num_classes,
        lr=cfg.opt.learning_rate,
        weight_decay=cfg.opt.weight_decay,
        lr_steps=getattr(cfg.opt, 'steps', None),  # å…¼å®¹v22ç‰ˆæœ¬å¯èƒ½æ²¡æœ‰stepså‚æ•°
        warmup_steps=cfg.opt.warmup_steps,
        ignored_index=cfg.dataset.ignored_classes_metric,
        output_aux_tokens=False,
    )
    return pl_module

